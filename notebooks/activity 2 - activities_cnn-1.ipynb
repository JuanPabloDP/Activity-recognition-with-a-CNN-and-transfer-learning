{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-xw1OHR5Psy"
   },
   "source": [
    "**Professor:** Enrique Garcia Ceja\n",
    "**email:** enrique.gc@tec.mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igYgjF3z5PON"
   },
   "source": [
    "# Exercise: Activity recognition with a CNN and transfer learning.\n",
    "\n",
    "In this exercise you will train a CNN to classify human acitivities from accelerometer sensors collected with a smartphone. You will be using the human activity recognition WISDM dataset (http://www.cis.fordham.edu/wisdm/dataset.php) which contains 6 different activities: walking, jogging, walking upstairs, walking downstairs, sitting and standing. The data was collected with an Android phone located in the participants' pants pocket. You will also use transfer learning to fine tune your model and adapt it to a specific user.\n",
    "\n",
    "The data was collected with a sampling rate of 20Hz (1 sample every 50ms). The accelerometer sensor captures values from three orientations (x,y and z). The picture below shows an example of acceleration values over time while a participant was walking.\n",
    "\n",
    "<table><tr><td><img src=\"https://github.com/enriquegit/ap-img/blob/main/img/cel.png?raw=true\" width=\"200\"></td><td><img src=\"https://github.com/enriquegit/ap-img/blob/main/img/walking.png?raw=true\" width=\"400\"></td></tr></table>\n",
    "\n",
    "From the picture above, it can be seen that we are dealing with time series data. But how can we train a CNN with this type of data? You can transform your data into an image-like representation! The timeseries data can be shaped into a matrix-like structure. See image below. In the example, the timeseires have a length of 9 and they are reshaped into 3x3 matrices. This can be seen as a RGB image (red, green, blue) but in this case it is a XYZ image! The time series values will be treated as if they were image pixels.\n",
    "\n",
    "<img src=\"https://github.com/enriquegit/ap-img/blob/main/img/representation.png?raw=true\" width=\"750\">\n",
    "\n",
    "The dataset has already been converted into image-like representations. The timeseries were split into five seconds segments (non-overlapping). Since the sampling rate was 20Hz, we will have arrays of length 100 for each x, y and z. The arrays will be reshaped into 10x10 images. Since we have x, y and z, each instance will be an image with dimension 10x10x3. Each row in the file images.txt contains the five second segments for all x, y and z values flattened. That is, the first 100 values correspond to x, the next 100 values to y and the last 100 values to z. The last two columns are the userid and label, respcectively.\n",
    "\n",
    "**In this exercise you will:**\n",
    "\n",
    "* Select a target user and build a model for her/him.\n",
    "* Train a general (user independent) CNN model.\n",
    "* Use 50% of the target user data as test set. Use the remaining 50% to build an adaptive model by fine tuning the general model using transfer learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jIWhFTh45POO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emn-Ohtt5POP"
   },
   "outputs": [],
   "source": [
    "# Path to the dataset.\n",
    "filepath = \"datasets/images.txt\"\n",
    "\n",
    "# We will save the trained CNN model into a file. Specify the path where the model will be saved.\n",
    "savemodelpath = \"datasets/\" # Make sure the directory already exists in your local computer.\n",
    "\n",
    "# Define the size of the images.\n",
    "img_width = 10\n",
    "img_height = 10\n",
    "img_depth = 3\n",
    "\n",
    "# Read the dataset.\n",
    "dataset = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJg6M3WD5POP"
   },
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the dataset.\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0CK8FUw5POP"
   },
   "outputs": [],
   "source": [
    "# Select features 'pixels' (acceleration values), userids,and class.\n",
    "pixels = dataset.drop(['userid','label'], axis=1)\n",
    "userids = dataset[['userid']]\n",
    "labels = dataset[['label']]\n",
    "\n",
    "# Convert to numpy.\n",
    "pixels = pixels.values\n",
    "userids = userids.values\n",
    "labels = labels.values\n",
    "\n",
    "# Encode the labels from strings to integers.\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels = le.fit_transform(labels.ravel())\n",
    "\n",
    "# Reshape pixels into images of 10x10x3.\n",
    "images = np.reshape(pixels, (pixels.shape[0],img_width,img_height,img_depth), order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvlEN5vH5POP"
   },
   "source": [
    "Now the `images` variable contains all our 10x10x3 images. The `userids` and `labels` variables contain the user ids and activity labels, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTnzCkqT5POP"
   },
   "outputs": [],
   "source": [
    "# Print the shapes of our data.\n",
    "print(userids.shape)\n",
    "print(labels.shape)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxzNKd575POP"
   },
   "outputs": [],
   "source": [
    "# Shuffle the data.\n",
    "\n",
    "def shuffle_instances(features, userids, labels):\n",
    "    n = features.shape[0]\n",
    "    np.random.seed(1234)\n",
    "    idxs = np.random.choice(n, size=n, replace=False)\n",
    "    features = features[idxs,:,:,:]\n",
    "    userids = userids[idxs]\n",
    "    labels = labels[idxs]\n",
    "\n",
    "    return(features, userids, labels)\n",
    "\n",
    "images, userids, labels = shuffle_instances(images, userids, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmsjSL1e5POP"
   },
   "source": [
    "Let's plot some of the images! The following code will print some warnings since the images are not exactly between 0-255 because strictly speaking, they are not images but the visualizations will still be useful. We can see that for low energy activities such as sitting or standing, the colors look very solid. On the other hand, for the jogging activity, the images look more fragmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1a2E91C5POQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(le.inverse_transform(labels)[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPqlmjc25POQ"
   },
   "outputs": [],
   "source": [
    "# Print the available user ids.\n",
    "with np.printoptions(threshold=np.inf):\n",
    "    print(np.unique(userids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHRFr8Wd5POQ"
   },
   "outputs": [],
   "source": [
    "# Select a target user from the above ids. This is the user we will use to test the general model.\n",
    "# We will also adapt the general model to this target user.\n",
    "\n",
    "target_user =  ### COMPLETE THE CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SnNDmNW5POQ"
   },
   "outputs": [],
   "source": [
    "# Split into training test sets.\n",
    "# The train set will be the data from all users except target_user.\n",
    "# The train set will be stored in images_all and labels_all.\n",
    "# The test set will be the data that belongs to target_user and is stored in images_target and labels_target.\n",
    "\n",
    "# Get indices\n",
    "idxs_target = np.where(userids == target_user)[0]\n",
    "idxs_all = np.where(userids != target_user)[0]\n",
    "\n",
    "images_all = images[idxs_all]\n",
    "labels_all = labels[idxs_all]\n",
    "\n",
    "images_target = images[idxs_target]\n",
    "labels_target = labels[idxs_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf8pztpb5POQ"
   },
   "outputs": [],
   "source": [
    "# Normalize images using the maximum value from the training data (images_all).\n",
    "\n",
    "# First, get the maximum value from images_all.\n",
    "# The max() function from numpy returns the maximum value of a numpy array.\n",
    "maxval =  #### COMPLETE THE CODE ####\n",
    "\n",
    "# Normalize images_all using maxval.\n",
    "images_all =  #### COMPLETE THE CODE ####\n",
    "\n",
    "# Normalize images_target using maxval.\n",
    "images_target =  #### COMPLETE THE CODE ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPn4Wk-g5POQ"
   },
   "source": [
    "Now we need a train and test set for the target user. The training set is used to fine tune the general model. We will use 50% of the target user's data to adapt the general model and the remaining to test the performance.\n",
    "\n",
    "Randomly split the target_user data (`images_target` `labels_target`) and assign 50% of the images and labels into variables defined as `images_target_train` and `labels_target_train` respectively. Store the remaining 50% of the images and labels in `images_target_test` and `labels_target_test`. Tip: You can use the `train_test_split()` function from scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-0oGncI5POQ"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bnyr0YCP5POQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the dimensions of our datasets.\n",
    "print(images_target_train.shape)\n",
    "print(labels_target_train.shape)\n",
    "\n",
    "print(images_target_test.shape)\n",
    "print(labels_target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ_-34Ib5POQ"
   },
   "source": [
    "Now build a CNN model. Since the images are small, you are advised to use a small kernel size, e.g., (2,2). Remember that you can use dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJdqm6Kk5POQ"
   },
   "outputs": [],
   "source": [
    "# Define your CNN general model here using keras functional API and name it 'model'.\n",
    "# The model takes as input images and predicts the activity type.\n",
    "# You should assign a unique name to each layer with the 'name' attribute.\n",
    "# This will later allow you to select specific layers by name.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFi_fg_j5POQ"
   },
   "outputs": [],
   "source": [
    "# Print a summary of your model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGtWkWvf5POR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define an optimizer and its learning rate.\n",
    "optimizer =  #### COMPLETE THE CODE ####\n",
    "\n",
    "# Set the batch size.\n",
    "batch_size =   #### COMPLETE THE CODE ####\n",
    "\n",
    "# Set number of epochs.\n",
    "num_epochs =   #### COMPLETE THE CODE ####\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# The general model is fitted with data from all users except the target user (images_all, labels_all).\n",
    "\n",
    "#### COMPLETE THE CODE ####\n",
    "history = model.fit(, ,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_split=0.15,\n",
    "                    verbose=1)\n",
    "\n",
    "# Save model after it has been fitted.\n",
    "model.save(savemodelpath + 'my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgKKd92d5POR"
   },
   "outputs": [],
   "source": [
    "# plot history\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bcupo7j5POR"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the target user's test data and print accuracy.\n",
    "# You can use the model.evaluate() function.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCsbGE7Q5POR"
   },
   "source": [
    "### Apply transfer learning\n",
    "\n",
    "Now, let's load the general model and adapt it to the target user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YgbXMAU5POR"
   },
   "outputs": [],
   "source": [
    "# Load the general model previously trained.\n",
    "model = keras.models.load_model(savemodelpath + 'my_model.keras')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQ1t2CCD5POR"
   },
   "outputs": [],
   "source": [
    "# Freeze some of the layers of the genral model to apply transfer learning.\n",
    "# Each layer has a 'trainable' attribute that you can set to False.\n",
    "# When you re-fit a model, layers whose trainable attribute is False will be untouched.\n",
    "# You can retrieve a layer with model.get_layer(\"layerName\")\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "\n",
    "# Define an optimizer and its learning rate.\n",
    "optimizer2 = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Compile the model again to apply changes.\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer2,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kazzb9o85POR"
   },
   "outputs": [],
   "source": [
    "# Print summary.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzAMOXAz5POR"
   },
   "source": [
    "After freezing some layers you can note that the number of trainable parameters was reduced. Now let's fine tune the model with the target user's train set: `images_target_train` `labels_target_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klon9afC5POR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### COMPLETE THE CODE ####\n",
    "# Since the target train set is very small, you can set validation_split = 0.0\n",
    "\n",
    "history = model.fit(, ,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = 15, # You can change this as well.\n",
    "                    validation_split = ,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAroqsqI5POR"
   },
   "outputs": [],
   "source": [
    "# Evaluate the adapted model on the test set (images_target_test) and print accuracy.\n",
    "\n",
    "#### YOUR CODE HERE ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zppP7UfQ5POR"
   },
   "source": [
    "After fine tunning your model the accuracy should be higher now!\n",
    "\n",
    "**This is the end of the exercise.**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
